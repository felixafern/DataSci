{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting **goes through cycles to iteratively add models into an ensemble**\n",
    "\n",
    "1. First, it initialises the ensemble with a single model \n",
    "2. Then, the ensemble is used to generate predictions. Each prediction is a function of all ensemble model's predictions\n",
    "3. Error is calculated with a loss function\n",
    "4. Loss functio is used to fit another model with parameters tweaked to reduce the loss\n",
    "5. Add the model to the ensemble and repeat the cycle\n",
    "\n",
    "Gradient in gradient boosting refers to the gradient descent that is done to mimise the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "data = pd.read_csv(\"/Users/felix/GitHub/DataSci/ml/KaggleML/melb_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "       validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting X and y as a subset of data\n",
    "X = data[['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']]\n",
    "y = data.Price\n",
    "\n",
    "# Separating into training and testing\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Instantiation and fitting\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error is \t 247544.4160472662\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "predictions = model.predict(X_valid)\n",
    "\n",
    "#Â Evaluating error\n",
    "print(f\"Mean absolute error is \\t {str(mean_absolute_error(predictions, y_valid))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters in XBGRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_n.estimators_**: Number of models included in the cycle. Typically betwen 100-1000\n",
    "\n",
    "**_early stopping rounds_**: Ceases iterations when the improvements are 0. Smart to set n_estimators high and then use early stopping rounds to find the optimal time to stop iterating \n",
    "\n",
    "**_learning rate_**: Multiplies models scores by a number so that each tree we add helps us less, allowing us to set high values for n estimators without overfitting. In general, small learning rate and high no. estimators yields an accurate model (but it takes longer to train)\n",
    "\n",
    "**_n jobs_**: Uses parallelism to train models faster. Set equal to the number of cores on the machine. No point on smaller datasets. Doesn't actually improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=1000, n_jobs=4, num_parallel_tree=1,\n",
       "       objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "       validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is how the parameters are set\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
